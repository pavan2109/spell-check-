{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from inflection import singularize\n",
    "from loguru import logger\n",
    "from textdistance import damerau_levenshtein\n",
    "\n",
    "from app.core.config import SPELLCHECK_MODEL_THRESH\n",
    "from app.db.events import mongo_db\n",
    "from app.db.repositories.mongo import MongoBaseRepository\n",
    "from app.resources.spellcheck import data\n",
    "from app.schema.spellcheck import SpellCheckSuccessResponse\n",
    "from app.services.normalize import normalize\n",
    "from app.services.spellcheck import clean, get_prediction\n",
    "\n",
    "\n",
    "async def get_spell_correction(query: str) -> SpellCheckSuccessResponse:\n",
    "    mongo_repo = MongoBaseRepository(mongo_db.client)\n",
    "    logger.info(f\"Search Query: {query}\")\n",
    "    # Cleaning\n",
    "    text = clean(query)\n",
    "    logger.info(f\"Cleaned query: {text}\")\n",
    "    if len(text) == 0:\n",
    "        return SpellCheckSuccessResponse(\n",
    "            query=query, corrected_query=\"\", confident=True\n",
    "        )\n",
    "    # First layer of overrides\n",
    "    mres = await mongo_repo._log_and_query_overrides(text)\n",
    "    if len(mres) > 0:\n",
    "        text = mres[0]\n",
    "        logger.info(f\"Overrided keyword: {text}\")\n",
    "    # Merging keywords\n",
    "    space_correct_txt = (\n",
    "        data.sym_spell_merge.word_segmentation(text)\n",
    "        .corrected_string.replace(\"&\", \" &\")\n",
    "        .split()\n",
    "    )\n",
    "    if len(space_correct_txt) < len(text.split()):\n",
    "        tokens = space_correct_txt\n",
    "        logger.info(f\"Merged space mistake: {text}->{' '.join(tokens)}\")\n",
    "    else:\n",
    "        tokens = text.split()\n",
    "\n",
    "    # Applying overrrdes\n",
    "    text = \" \".join(tokens)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        mres = await mongo_repo._log_and_query_overrides(token)\n",
    "        if len(mres) > 0:\n",
    "            text = text.replace(token, mres[0])\n",
    "            logger.info(f\"Word overrided: {token}->{mres[0]}\")\n",
    "    for phrase in nltk.everygrams(text.split(), min_len=2):\n",
    "        mres = await mongo_repo._log_and_query_overrides(\" \".join(phrase))\n",
    "        if len(mres) > 0:\n",
    "            text = text.replace(\" \".join(phrase), mres[0])\n",
    "            logger.info(f\"Phrase overrided: {' '.join(phrase)}->{mres[0]}\")\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Splitting alpha words and alphanumerics\n",
    "    clean_tokens = []\n",
    "    removed_idx = []\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.isalpha():\n",
    "            clean_tokens.append(token)\n",
    "        # elif len(token) < 3:\n",
    "        #     removed_idx.append(idx)\n",
    "        else:\n",
    "            removed_idx.append(idx)\n",
    "    if len(clean_tokens) == 0:\n",
    "        logger.info(\"No clean words present in the query\")\n",
    "        return SpellCheckSuccessResponse(\n",
    "            query=query, corrected_query=\" \".join(tokens), confident=True\n",
    "        )\n",
    "    # Single word case\n",
    "    clean_text = \" \".join(clean_tokens)\n",
    "    if len(clean_tokens) == 1:\n",
    "        in_vocab = False\n",
    "        if clean_tokens[0] in data.vocab:\n",
    "            logger.info(\"Clean unigram already present in vocab\")\n",
    "            in_vocab = True\n",
    "        # Single word present in vocabulary case\n",
    "        if in_vocab:\n",
    "            if len(removed_idx) > 0:\n",
    "                for idx in removed_idx:\n",
    "                    clean_tokens.insert(idx, tokens[idx])\n",
    "            normalized_txt = await normalize(\" \".join(clean_tokens))\n",
    "            mres = await mongo_repo._log_and_query_high_recall(normalized_txt)\n",
    "            high_recall = False\n",
    "            if len(mres) > 0:\n",
    "                high_recall = True\n",
    "            return SpellCheckSuccessResponse(\n",
    "                query=query,\n",
    "                corrected_query=\" \".join(clean_tokens),\n",
    "                normalized_query=normalized_txt,\n",
    "                confident=True,\n",
    "                high_recall=high_recall,\n",
    "            )\n",
    "\n",
    "    # Checking for Protected and overrides for exact match\n",
    "    mres = await mongo_repo._log_and_query_protected(clean_text)\n",
    "    if len(mres) > 0:\n",
    "        clean_tokens = clean_text.split()\n",
    "        logger.info(f\"Protected keyword: {clean_text}\")\n",
    "        if len(removed_idx) > 0:\n",
    "            for idx in removed_idx:\n",
    "                clean_tokens.insert(idx, tokens[idx])\n",
    "        normalized_txt = await normalize(\" \".join(clean_tokens))\n",
    "        mres = await mongo_repo._log_and_query_high_recall(\" \".join(clean_tokens))\n",
    "        high_recall = False\n",
    "        if len(mres) > 0:\n",
    "            high_recall = True\n",
    "        return SpellCheckSuccessResponse(\n",
    "            query=query,\n",
    "            corrected_query=\" \".join(clean_tokens),\n",
    "            normalized_query=normalized_txt,\n",
    "            confident=True,\n",
    "            high_recall=high_recall,\n",
    "        )\n",
    "    mres = await mongo_repo._log_and_query_overrides(clean_text)\n",
    "    if len(mres) > 0:\n",
    "        clean_text = mres[0]\n",
    "        clean_tokens = clean_text.split()\n",
    "        logger.info(f\"Overrided keyword: {clean_text}\")\n",
    "        if len(removed_idx) > 0:\n",
    "            for idx in removed_idx:\n",
    "                clean_tokens.insert(idx, tokens[idx])\n",
    "        normalized_txt = await normalize(\" \".join(corrected_tokens))\n",
    "        mres = await mongo_repo._log_and_query_high_recall(\" \".join(clean_tokens))\n",
    "        high_recall = False\n",
    "        if len(mres) > 0:\n",
    "            high_recall = True\n",
    "        return SpellCheckSuccessResponse(\n",
    "            query=query,\n",
    "            corrected_query=\" \".join(clean_tokens),\n",
    "            normalized_query=normalized_txt,\n",
    "            confident=True,\n",
    "            high_recall=high_recall,\n",
    "        )\n",
    "    # Checking protected for partial match\n",
    "    correct_idx = []\n",
    "    for idx, token in enumerate(clean_tokens):\n",
    "        mres = await mongo_repo._log_and_query_protected(token)\n",
    "        if len(mres) > 0:\n",
    "            correct_idx.append(idx)\n",
    "            continue\n",
    "        if len(token) < 3:\n",
    "            correct_idx.append(idx)\n",
    "    if len(correct_idx) == len(clean_tokens):\n",
    "        if len(removed_idx) > 0:\n",
    "            for idx in removed_idx:\n",
    "                clean_tokens.insert(idx, tokens[idx])\n",
    "        normalized_txt = await normalize(\" \".join(clean_tokens))\n",
    "        mres = await mongo_repo._log_and_query_high_recall(normalized_txt)\n",
    "        high_recall = False\n",
    "        if len(mres) > 0:\n",
    "            high_recall = True\n",
    "        return SpellCheckSuccessResponse(\n",
    "            query=query,\n",
    "            corrected_query=\" \".join(clean_tokens),\n",
    "            normalized_query=normalized_txt,\n",
    "            confident=True,\n",
    "            high_recall=high_recall,\n",
    "        )\n",
    "\n",
    "    # Spltting merged words\n",
    "    space_corrected_tokens = clean_tokens.copy()\n",
    "    offset = 0\n",
    "    for i in range(len(clean_tokens)):\n",
    "        if i not in correct_idx and clean_tokens[i] not in data.vocab:\n",
    "            res = data.sym_spell_split.word_segmentation(\n",
    "                clean_tokens[i],\n",
    "                max_edit_distance=0,\n",
    "            ).corrected_string\n",
    "            if (\n",
    "                clean_tokens[i] != res\n",
    "                and len(await mongo_repo._log_and_query_ngrams(res)) > 0\n",
    "                and all([len(w) > 1 for w in res.split()])\n",
    "            ):\n",
    "                space_corrected_tokens[i + offset] = res.split()[0]\n",
    "                for idx, w in enumerate(res.split()[1:]):\n",
    "                    space_corrected_tokens.insert(i + offset + 1 + idx, w)\n",
    "                idx = tokens.index(clean_tokens[i])\n",
    "                tokens[idx] = res.split()[0]\n",
    "                for idx_, w in enumerate(res.split()[1:]):\n",
    "                    tokens.insert(idx + 1 + idx_, w)\n",
    "                for j in range(len(removed_idx)):\n",
    "                    if removed_idx[j] > idx:\n",
    "                        removed_idx[j] += len(res.split()) - 1\n",
    "                offset += len(res.split()) - 1\n",
    "                continue\n",
    "            elif len(clean_tokens) == 1:\n",
    "                res = data.sym_spell_split.lookup(\n",
    "                    clean_tokens[i], max_edit_distance=2, verbosity=1\n",
    "                )\n",
    "                if len(res) > 0:\n",
    "                    space_corrected_tokens[i] = res[0].term\n",
    "                    continue\n",
    "            res = data.sym_spell_split.word_segmentation(\n",
    "                clean_tokens[i],\n",
    "                max_edit_distance=1,\n",
    "            ).corrected_string\n",
    "            if clean_tokens[i] != res and (\n",
    "                len(await mongo_repo._log_and_query_ngrams(res)) > 0\n",
    "                and all([len(w) > 1 for w in res.split()])\n",
    "                or (len(clean_tokens) == 1 and res in data.vocab)\n",
    "            ):\n",
    "                space_corrected_tokens[i + offset] = res.split()[0]\n",
    "                for idx, w in enumerate(res.split()[1:]):\n",
    "                    space_corrected_tokens.insert(i + offset + 1 + idx, w)\n",
    "                idx = tokens.index(clean_tokens[i])\n",
    "                tokens[idx] = res.split()[0]\n",
    "                for idx_, w in enumerate(res.split()[1:]):\n",
    "                    tokens.insert(idx + 1 + idx_, w)\n",
    "                for j in range(len(removed_idx)):\n",
    "                    if removed_idx[j] > idx:\n",
    "                        removed_idx[j] += len(res.split()) - 1\n",
    "                offset += len(res.split()) - 1\n",
    "\n",
    "    if space_corrected_tokens != clean_tokens:\n",
    "        clean_tokens = space_corrected_tokens\n",
    "        confidence = [1.0]\n",
    "        logger.info(f\"Symspell correction: {' '.join(space_corrected_tokens)}\")\n",
    "    # Spell Correction by Model\n",
    "    res = await get_prediction(\" \".join(clean_tokens))\n",
    "    corrected_tokens, score_, confidence = (\n",
    "        [i[\"correction\"] for i in res[0]],\n",
    "        [i[\"score\"] for i in res[0]],\n",
    "        [i[\"confidence\"] for i in res[0]],\n",
    "    )\n",
    "    logger.info(f\"Model correction: {' '.join(corrected_tokens)}\")\n",
    "    for i in range(len(clean_tokens)):\n",
    "        if singularize(corrected_tokens[i]) == clean_tokens[i] or singularize(\n",
    "            corrected_tokens[i]\n",
    "        ) == singularize(clean_tokens[i]):\n",
    "            corrected_tokens[i] = clean_tokens[i]\n",
    "\n",
    "    if len(correct_idx) > 0:\n",
    "        for idx in correct_idx:\n",
    "            corrected_tokens[idx] = clean_tokens[idx]\n",
    "            # confidence[idx] = 1.0\n",
    "    if len(removed_idx) > 0:\n",
    "        for idx in removed_idx:\n",
    "            corrected_tokens.insert(idx, tokens[idx])\n",
    "            # confidence.insert(idx, 1.0)\n",
    "    # Calculating confidence\n",
    "    is_confident = (\n",
    "        True\n",
    "        if damerau_levenshtein(\" \".join(corrected_tokens), \" \".join(clean_tokens)) < 3\n",
    "        and all([i > SPELLCHECK_MODEL_THRESH for i in confidence])\n",
    "        else False\n",
    "    )\n",
    "\n",
    "    normalized_txt = await normalize(\" \".join(corrected_tokens))\n",
    "\n",
    "    mres = await mongo_repo._log_and_query_high_recall(normalized_txt)\n",
    "    high_recall = False\n",
    "    if len(mres) > 0:\n",
    "        high_recall = True\n",
    "    return SpellCheckSuccessResponse(\n",
    "        query=query,\n",
    "        corrected_query=\" \".join(corrected_tokens),\n",
    "        normalized_query=normalized_txt,\n",
    "        confident=is_confident,\n",
    "        high_recall=high_recall,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
